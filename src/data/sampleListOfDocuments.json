[
    {
        "_id": "1",
        "title": "Data Science in Action",
        "abstract" : "In recent years, data science emerged as a new and important discipline. It can be viewed as an amalgamation of classical disciplines like statistics, data mining, databases, and distributed systems. Existing approaches need to be combined to turn abundantly available data into value for individuals, organizations, and society. Moreover, new challenges have emerged, not just in terms of size (“Big Data”) but also in terms of the questions to be answered. This book focuses on the analysis of behavior based on event data. Process mining techniques use event data to discover processes, check compliance, analyze bottlenecks, compare process variants, and suggest improvements. In later chapters, we will show that process mining provides powerful tools for today’s data scientist. However, before introducing the main topic of the book, we provide an overview of the data science discipline.",
        "publishYear": "2016",
        "authors":[{
            "name":"Wil van der Aalst"
        }],
        "relevancy":"10",
        "citing": "150",
        "cited" : "60"
    },
    {
        "_id": "2",
        "title": "Data science and prediction",
        "abstract" : "Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.",
        "publishYear": "2013",
        "authors":[{
            "name":"Vasant Dhar"
        }],
        "relevancy":"6",
        "citing": "100",
        "cited" : "120"
    },
    {
        "_id": "3",
        "title": "Data Science and its Relationship to Big Data and Data-Driven Decision Making",
        "abstract" : "Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot—even “sexy”—career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",
        "publishYear": "2013",
        "authors":[{
            "name":"Foster Provost"
        },{
            "name":"Tom Fawcett"
        }],
        "relevancy":"7",
        "citing": "90",
        "cited" : "10"
    },
    {
        "_id": "4",
        "title": "Data Analysis with the SolarSoft System",
        "abstract" : "The SolarSoftWare (SSW) system is a set of integrated software libraries, databases and system utilities which provide a common programming and data analysis environment for solar physics. Primarily an IDL based system, SSW is a collection of common data management and analysis routines derived from the Yohkoh and SOHO missions, the Solar Data Analysis Center, the astronomy libraries and other packages. The SSW environment is designed to provide a consistent look and feel at co-investigator institutions and facilitate sharing and exchange of data. The SSW system minimizes the learning curve when doing research away from the home institution or when correlating results from multiple experiments.",
        "publishYear": "1998",
        "authors":[{
            "name":"S.L. Freeland"
        },{
            "name" : "B.N. Handy"
        }],
        "relevancy":"8",
        "citing": "50",
        "cited" : "200"
    },
    {
        "_id": "5",
        "title": "Geodetic datum transformation to the global geocentric datum for seas and islands around Korea",
        "abstract" : "According to revisions of survey law taking effect on January 1, 2003, the Korean geodetic datum has been changed from a local geodetic to a world geodetic system. Since the datum change demands a geographical data transformation, the National Geographic Information Institute has established step-by-step plans for the transformation of the land data constructed through the National GIS Project, and it is in progress. For maritime data, however, no detailed transformation plan has been established yet. Therefore, it is necessary to analyze the maritime geographic data obtained through the Maritime GIS project and set up the data transformation scheme to a world geodetic system. In this study, the datum transformation parameters especially for the maritime geographical data are determined. From database constructed through MGIS, a total of 492 coordinate pairs were used in parameter determination initially. At this stage, three popular seven parameter transformation models, Bursa-Wolf, Molodensky and Veis model, and the multi regression equation are applied, and the transformation parameters from the Molodensky model are selected for its accuracy and consistency with the land data transformation method. To eliminate the local bias caused by the nonequally distributed stations, a network optimization is applied and 42 stations are selected to determine the final transformation parameters. The distortion after applying the similarity transformation is modeled through a least squares collocation with Gaussian model, and high accuracy better than 15 cm in coordinate transformation is obtained.",
        "publishYear": "2005",
        "authors":[{
            "name":"Jay Hyoun Kwon"
        },{
            "name":"Tae-Suk Bae"
        },{
            "name":"Yoon-Soo Choi"
        },{
            "name":"Dong-Cheon Lee"
        },{
            "name":"Young-Wook Lee"
        }],
        "relevancy":"7",
        "citing": "85",
        "cited" : "150"
    },
    {
        "_id": "6",
        "title": "The study and analysis of floating datum selection for rugged terrain",
        "abstract" : "We analyze the characteristics of different floating datums for static corrections and discuss the methods for determining them. The effect of different floating datum corrections was studied using theoretical model experiments, resulting in the conclusion that the velocity obtained after the floating datum correction with the minimum static correction errors depends on the velocity of the layer below the low velocity layer (LVL) lower boundary and is not related to topographic relief and LVL structure. For the real data processing case, wave equation numerical model experiments were conducted which resulted in a new method for calculating objective functions based on the waveform and modifications to the calculation equation for minimum static correction errors to make the method suitable for real data static correction processing using inhomogeneous velocity models with lower velocity boundary relief. Real data processing results demonstrate the method’s superiority.",
        "publishYear": "2007",
        "authors":[{
            "name":"Jiao He"
        },
        {
            "name":"Yuzhu Liu"
        },{
            "name":"Jianhua Geng"
        }],
        "relevancy":"10",
        "citing": "98",
        "cited" : "130"
    },
    {
        "_id": "7",
        "title": "Induction-by-stages of Dixmier algebras and orbit datum and its application",
        "abstract" : "The purpose of the present note is to prove that the parabolic inductions of the orbit datum and Dixmier algebras can be induced by stages. And as an appliction, we have proved that forSO(2n + 1,C),SP(2n. C).F 4 andG 2, the inductions of complete prime Abel orbit datum are independent of the Choice of parabolic subgroups.",
        "publishYear": "1998",
        "authors":[{
            "name":"Chinese Science Bulletin"
        }],
        "relevancy":"9",
        "citing": "120",
        "cited" : "350"
    },
    {
        "_id": "8",
        "title": "The ongoing modernization of the Taiwan semi-dynamic datum based on the surface horizontal deformation model using GNSS data from 2000 to 2016",
        "abstract" : "A semi-dynamic geodetic datum, composed of a static geodetic datum and a surface deformation model, is proposed in this study to maintain the accuracy of geodetic datum in Taiwan. A concept to construct the surface deformation model is also suggested to accommodate the characteristics of temporal variation of the velocity field and coseismic displacements caused by earthquakes in Taiwan. In this study, we proposed a surface deformation model, containing a secular velocity grid model during 2000–2016 and a coseismic displacement grid model of the 2016 ML 6.6 Meinong earthquake, as an example to examine its adaptability in Taiwan. The secular velocity field relative to the station KMNM from 2000 to 2016 was first evaluated in this study using data from 380 continuous GNSS stations in Taiwan. Integrating 672 campaign-mode GNSS velocities from 2002 to 2016, a secular velocity grid model was constructed using the Kriging interpolation method. The high-precision coseismic displacements of the 2016 Meinong earthquake calculated using the IGS ultra-rapid orbit were also evidenced. The coseismic displacement grid model for all Taiwan for this event was built using the kinematic dislocation model to prevent contamination from nontectonic sources. Another 1341 independent GNSS control points surveyed in 2013 and 2016 were adopted to validate the reliability of the surface deformation model. After correction by the deformation model, 1219 points (91%) matched the criterion at the urban region of cadastral surveying in Taiwan (< 6 cm). The Electronic Global Navigation Satellite System (e-GNSS) in Taiwan is suggested to be integrated with the geodetic semi-dynamic datum to improve the precision of e-GNSS and to monitor the accuracy of the surface deformation model in the Taiwan semi-dynamic datum.",
        "publishYear": "2019",
        "authors":[{
            "name":"Chien-Kuan Li"
        },{
            "name":"Kuo-En Ching"
        },{
            "name":"Kwo-Hwa Chen"
        }],
        "relevancy":"7",
        "citing": "150",
        "cited" : "3"
    },
    {
        "_id": "9",
        "title": "Datum unit optimization for robustness of a journal bearing diagnosis system",
        "abstract" : "This paper describes a robust diagnosis method for a rotor system with a journal bearing. To enhance the robustness of a journal bearing diagnosis system, it is of great importance to define an optimum datum unit for featuring anomaly states of the rotor system. To support the research goal, this study makes use of three measures for class separation, including Kullback-Leibler divergence (KLD), Fisher discriminant ratio (FDR), and a newly proposed measure: probability of separation (PoS). From the viewpoint of class separability, this work found that PoS is more attractive than other methods for quantification of class separation. PoS offers favorable properties like normalization, boundedness, and high sensitivity. A generic algorithm integrated with one of three measures consistently suggested the optimum datum units among the feasible datum units. Optimum datum units were found to be one-cycle for time-domain features and sixty-cycles for frequency-domain features. The support vector machine (SVM) classifier with the optimum datum units was used for diagnosing a normal and three anomaly states. The health classification results showed that the proposed optimum datum units can outperform other datum units.",
        "publishYear": "2015",
        "authors":[{
            "name":"Byung Chul Jeon"
        },{
            "name":"Joon Ha Jung"
        },{
            "name":"Byeng Dong Youn"
        },{
            "name": "Yeon-Whan Kim"
        },{
            "name":"Yong-Chae Bae"
        }],
        "relevancy":"8",
        "citing": "160",
        "cited" : "35"
    },
    {
        "_id": "10",
        "title": "An approach to the analysis of the underlying structure of visual space using a generalized notion of visual pattern recognition",
        "abstract" : "An approach is described by which certain deductions can be made concerning the natural underlying structure of visual space. The procedure is indirect and makes use of the notion of visual recognition defined with respect to an arbitrarily fixed structure. Consideration of the set of mappings associated with such recognition is shown to lead to a condition that must be satisfied by any proposed underlying structure.",
        "publishYear": "1975",
        "authors":[{
            "name":"David H. Foster "
        }],
        "relevancy":"5",
        "citing": "80",
        "cited" : "100"
    },
    {
        "_id": "11",
        "title": "Resilient Machine Learning (rML) Ensemble Against Adversarial Machine Learning Attacks",
        "abstract" : "Machine Learning (ML) algorithms have been widely used in many critical applications, including Dynamic Data Driven Applications Systems (DDDAS) applications, automated financial trading applications, autonomous vehicles, and intrusion detection systems for the decision-making process of users or automated systems. However, malicious adversaries have strong interests in manipulation the operations of machine learning algorithms to achieve their objectives in gaining financially, injecting injury or disasters. Adversaries against ML can be classified based on their capabilities and goals into two types: Adversary who has full knowledge of the ML models and parameters (white-box scenario) and one that does not have any knowledge and use guessing techniques to figure out the ML model and its parameters (black-box scenario). In both scenarios, the adversaries will attempt to maliciously manipulate model either during training or testing. Defending against these attacks can be successful by following three methods: 1) making the ML model robust to adversary, 2) validating and verifying input, or 3) changing ML architecture. In this paper, we present a resilient machine learning (rML) ensemble against adversarial attacks by dynamically changing the ML architecture and the ML models to be used such that the adversaries have no knowledge about the current ML model being used and consequently stop their attempt to manipulate the ML operations at testing phase. We evaluate the effectiveness of our rML ensemble using the benchmarking, zero-query dataset “DAmageNet” that contains both clean and adversarial image samples. We use three main neural networks in our ensemble that includes VGG16, ResNet-50, and ResNet-101. The experimental results show that our rML can tolerate the adversarial samples and achieve high classification accuracy with small execution time degradation.",
        "publishYear": "2020",
        "authors":[{
            "name":"Likai Yao"
        },{
            "name":"Cihan Tunc"
        },{
            "name":"Pratik Satam"
        },{
            "name":"Salim Hariri"
        }],
        "relevancy":"6",
        "citing": "190",
        "cited" : "5"
    },
    {
        "_id": "12",
        "title": "Current Advances, Trends and Challenges of Machine Learning and Knowledge Extraction: From Machine Learning to Explainable AI",
        "abstract" : "In this short editorial we present some thoughts on present and future trends in Artificial Intelligence (AI) generally, and Machine Learning (ML) specifically. Due to the huge ongoing success in machine learning, particularly in statistical learning from big data, there is rising interest of academia, industry and the public in this field. Industry is investing heavily in AI, and spin-offs and start-ups are emerging on an unprecedented rate. The European Union is allocating a lot of additional funding into AI research grants, and various institutions are calling for a joint European AI research institute. Even universities are taking AI/ML into their curricula and strategic plans. Finally, even the people on the street talk about it, and if grandma knows what her grandson is doing in his new start-up, then the time is ripe: We are reaching a new AI spring. However, as fantastic current approaches seem to be, there are still huge problems to be solved: the best performing models lack transparency, hence are considered to be black boxes. The general and worldwide trends in privacy, data protection, safety and security make such black box solutions difficult to use in practice. Specifically in Europe, where the new General Data Protection Regulation (GDPR) came into effect on May, 28, 2018 which affects everybody (right of explanation). Consequently, a previous niche field for many years, explainable AI, explodes in importance. For the future, we envision a fruitful marriage between classic logical approaches (ontologies) with statistical approaches which may lead to context-adaptive systems (stochastic ontologies) that might work similar as the human brain.",
        "publishYear": "2018",
        "authors":[{
            "name":"Andreas Holzinger"
        },{
            "name":"Peter Kieseberg"
        },{
            "name":"Edgar Weippl"
        },{
            "name":"A Min Tjoa"
        }],
        "relevancy":"7",
        "citing": "60",
        "cited" : "15"
    },
    {
        "_id": "13",
        "title": "Machine Learning Unplugged - Development and Evaluation of a Workshop About Machine Learning",
        "abstract" : "Machine learning, being an important part of artificial intelligence, is increasingly discussed and rated in the media without explaining its functionality. This can lead to misconceptions of its real impact and range of application, a problem especially concerning young people. This contribution focuses on the theory-driven development and practical experience with an unplugged workshop concept, which is about a simple technique of machine learning, as a basis for possible teaching units for high school students. For this purpose, the focus of the workshop is an action-oriented method to simulate the classification of screws with two different lengths. Workshop participants can reconstruct linear classification by moving a classifier represented by a wooden strip according to defined rules after each insertion of training data on a pinboard. The aim is to examine whether and how the topic can be made understandable at school. Pre- and posttests are used to evaluate the impact of the workshop on the participants’ image of artificial intelligence and machine learning. The results of this research suggest that it is possible to reduce simple methods of machine learning for teaching this topic at school. Moreover, it seems that even a 90-min workshop can change the participants’ conceptions of machine learning and artificial intelligence to a more realistic appreciation of their impact.",
        "publishYear": "2019",
        "authors":[{
            "name":"Elisaweta Ossovski"
        },{
            "name":"Michael Brinkmeier"
        }],
        "relevancy":"8",
        "citing": "90",
        "cited" : "6"
    },
    {
        "_id": "14",
        "title": "Special Issue on Human-Computer Interaction",
        "abstract" : "This special issue on HCI reflects some of the main roads of current research in HCI. It represents a nice multifaceted mix of world-renowned experts from many different disciplines. A good starting point for reading is the very first article of this special issue, “Human-Computer Interaction—Introduction and Overview”, which takes the reader from research to practice, from computer science to cognitive psychology, and from past to future. One of the highlights of this issue is the interview with Turing award winner Alan Kay, who reflects on past, present and future of HCI.",
        "publishYear": "2012",
        "authors":[{
            "name":"Achim Ebert"
        }],
        "relevancy":"6",
        "citing": "45",
        "cited" : "23"
    },
    {
        "_id": "15",
        "title": "Retraction Note to: Human–computer interaction using vision-based hand gesture recognition systems: a survey",
        "abstract" : "Considerable effort has been put toward the development of intelligent and natural interfaces between users and computer systems. In line with this endeavor, several modes of information (e.g., visual, audio, and pen) that are used either individually or in combination have been proposed. The use of gestures to convey information is an important part of human communication. Hand gesture recognition is widely used in many applications, such as in computer games, machinery control (e.g., crane), and thorough mouse replacement. Computer recognition of hand gestures may provide a natural computer interface that allows people to point at or to rotate a computer-aided design model by rotating their hands. Hand gestures can be classified into two categories: static and dynamic. The use of hand gestures as a natural interface serves as a motivating force for research on gesture taxonomy, its representations, and recognition techniques. This paper summarizes the surveys carried out in human--computer interaction (HCI) studies and focuses on different application domains that use hand gestures for efficient interaction. This exploratory survey aims to provide a progress report on static and dynamic hand gesture recognition (i.e., gesture taxonomies, representations, and recognition techniques) in HCI and to identify future directions on this topic.",
        "publishYear": "2013",
        "authors":[{
            "name":"Haitham Hasan"
        },{
            "name": "Sameem Abdul-Kareem"
        }],
        "relevancy":"5",
        "citing": "80",
        "cited" : "60"
    },
    {
        "_id": "16",
        "title": "Vision based hand gesture recognition for human computer interaction: a survey",
        "abstract" : "As computers become more pervasive in society, facilitating natural human–computer interaction (HCI) will have a positive impact on their use. Hence, there has been growing interest in the development of new approaches and technologies for bridging the human–computer barrier. The ultimate aim is to bring HCI to a regime where interactions with computers will be as natural as an interaction between humans, and to this end, incorporating gestures in HCI is an important research area. Gestures have long been considered as an interaction technique that can potentially deliver more natural, creative and intuitive methods for communicating with our computers. This paper provides an analysis of comparative surveys done in this area. The use of hand gestures as a natural interface serves as a motivating force for research in gesture taxonomies, its representations and recognition techniques, software platforms and frameworks which is discussed briefly in this paper. It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition. Different application which employs hand gestures for efficient interaction has been discussed under core and advanced application domains. This paper also provides an analysis of existing literature related to gesture recognition systems for human computer interaction by categorizing it under different key parameters. It further discusses the advances that are needed to further improvise the present hand gesture recognition systems for future perspective that can be widely used for efficient human computer interaction. The main goal of this survey is to provide researchers in the field of gesture based HCI with a summary of progress achieved to date and to help identify areas where further research is needed.",
        "publishYear": "2012",
        "authors":[{
            "name":"Siddharth S. Rautaray"
        },{
            "name":"Anupam Agrawal"
        }],
        "relevancy":"7",
        "citing": "35",
        "cited" : "6"
    },
    {
        "_id": "17",
        "title": "Guest Editorial: Human–Computer Interaction: Real-Time Vision Aspects of Natural User Interfaces",
        "abstract" : "Computer vision for human–computer interaction has been in our living rooms for a few years now. We have witnessed an exponential growth in capability of vision systems for computer games, from Sony’s EyeToy for PlayStation 2 in 2003, Eye for PlayStation 3 in 2007, to Microsoft’s Kinect for XBOX 360 in 2010. In addition to enhancing our interaction with computer games, Kinect has had a deeper role in changing our expectations for human–machine interfaces of the future. We may need to wait a few more years to have computers like those in the science fiction movie Minority Report, but the games and hacks using Kinect show us that such interfaces are getting closer to reality. We now expect our TVs, smart phones, and tablets to have such capability and soon that will be a reality.",
        "publishYear": "2012",
        "authors":[{
            "name":"Zoran Živković"
        }, {
            "name": "Nicu Sebe"
        },{
            "name": "Hamid Aghajan"
        } , {
            "name": "Branislav Kisačanin"
        }],
        "relevancy":"10",
        "citing": "90",
        "cited" : "120"
    },
    {
        "_id": "18",
        "title": "Multimodal Interfaces of Human–Computer Interaction",
        "abstract" : "An analytical review of state-of-the-art and future intelligent interfaces of human–computer interaction is presented; stages of their evolution are considered from command text to graphic and then to intelligent uni- and multimodal interfaces, based on the transfer of acoustic, visual, textual, and neural information. The principles of organization and the main characteristics and types of multimodal user interfaces, which employ concurrently several tools for automatic processing (recognition and synthesis) of userinputted heterogeneous information, are detailed. The combination of computers with speech and multimodal interfaces, designed for user-friendly information input/output, creates universal information–communicative technologies, man coming to the fore in the interaction between man and computer. Russian and foreign developments in this field are analyzed briefly.",
        "publishYear": "2018",
        "authors":[{
            "name":"A. A. Karpov"
        }, {
            "name":"R. M. Yusupov"
        }],
        "relevancy":"8",
        "citing": "35",
        "cited" : "5"
    },
    {
        "_id": "19",
        "title": "Human-Computer Interaction Problem in Learning: Could the Key Be Hidden Somewhere Between Social Interaction and Development of Tools?",
        "abstract" : "Homo sapiens is not just a tool-using species, they also can invent and develop tools. This is the feature that distinguishes humans from other species. It is necessary to get rid of the perceptual dominance of the present state of the material to invent and develop a tool. It is based on a mental process: designing. So how was that possible? In this article, I propose an evolutionary hypothesis in response to this question: the referential triangle. Accordingly, the relationship that people establish with things is mentally indirect, but the relationship they establish with each other is mentally direct. The hypothesis claims that the mental solutions of people have naturally established with each other are also used to invent and develop tools. The latest and most interesting product of this mechanism is artificial intelligence. Because artificial intelligence also acts as an inorganic system. What distinguishes it from other machines in this context is its social behavior. Artificial intelligence can generate social signals. So can artificial intelligence be both a tool and a partner at the same time in the referential triangle established with the tools by the human? In other words, can children, for instance, socially interact with artificial intelligence, just as they do naturally with people around? The article draws attention to that this problem should be included in the cultural psychological research agenda.",
        "publishYear": "2019",
        "authors":[{
            "name":"Tolga Yıldız"
        }],
        "relevancy":"9",
        "citing": "90",
        "cited" : "12"
    },
    {
        "_id": "20",
        "title": "The Role of Cognitive Modeling for User Interface Design Representations: An Epistemological Analysis of Knowledge Engineering in the Context of Human-Computer Interaction",
        "abstract" : "In this paper we review some problems with traditional approaches for acquiring and representing knowledge in the context of developing user interfaces. Methodological implications for knowledge engineering and for human-computer interaction are studied. It turns out that in order to achieve the goal of developing human-oriented (in contrast to technology-oriented) human-computer interfaces developers have to develop sound knowledge of the structure and the representational dynamics of the cognitive system which is interacting with the computer. \n\n We show that in a first step it is necessary to study and investigate the different levels and forms of representation that are involved in the interaction processes between computers and human cognitive systems. Only if designers have achieved some understanding about these representational mechanisms, user interfaces enabling individual experiences and skill development can be designed. In this paper we review mechanisms and processes for knowledge representation on a conceptual, epistemological, and methodologieal level, and sketch some ways out of the identified dilemmas for cognitive modeling in the domain of human-computer interaction.",
        "publishYear": "1998",
        "authors":[{
            "name":"Markus F. Peschl"
        },{
            "name": "Chris Stary"
        }],
        "relevancy":"4",
        "citing": "45",
        "cited" : "115"
    },
    {
        "_id": "21",
        "title": "The Epistemology and Ontology of Human-Computer Interaction",
        "abstract" : "This paper analyzes epistemological and ontological dimensions of Human-Computer Interaction (HCI) through an analysis of the functions of computer systems in relation to their users. It is argued that the primary relation between humans and computer systems has historically been epistemic: computers are used as information-processing and problem-solving tools that extend human cognition, thereby creating hybrid cognitive systems consisting of a human processor and an artificial processor that process information in tandem. In this role, computer systems extend human cognition. Next, it is argued that in recent years, the epistemic relation between humans and computers has been supplemented by an ontic relation. Current computer systems are able to simulate virtual and social environments that extend the interactive possibilities found in the physical environment. This type of relationship is primarily ontic, and extends to objects and places that have a virtual ontology. Increasingly, computers are not just information devices, but portals to worlds that we inhabit. The aforementioned epistemic and ontic relationships are unique to information technology and distinguish human-computer relationships from other human-technology relationships.",
        "publishYear": "2005",
        "authors":[{
            "name":"PHILIP BREY"
        }],
        "relevancy":"5",
        "citing": "36",
        "cited" : "30"
    },
    {
        "_id": "22",
        "title": "The sorcerer and the apprentice. Human-computer interaction today",
        "abstract" : "Human-computer interaction today has got a touch of magic: Without understanding the causal coherence, using a computer seems to become the art to use the right spell with the mouse as the magic wand — the sorcerer's staff. Goethes's poem admits an allegoric interpretation. We explicate the analogy between using a computer and casting a spell with emphasis on teaching magic skills. The art to create an ergonomic user interface has to take care of various levels of skills for the human operators. The problem of logical discontinuities as opposed to continuous control is the most serious obstacle for human-computer interaction.",
        "publishYear": "1998",
        "authors":[{
            "name":"W. Oberschelp"
        }],
        "relevancy":"6",
        "citing": "56",
        "cited" : "50"
    },
    {
        "_id": "23",
        "title": "Human-Engaged Computing: the future of Human–Computer Interaction",
        "abstract" : "Debates regarding the nature and role of Human-Computer Interaction (HCI) have become increasingly common. This is because HCI lacks a clear philosophical foundation from which to derive a coherent vision and consistent aims and goals. This paper proposes a conceptual framework for ongoing discussion that can give more meaningful and pertinent direction to the future of HCI; we call the proposed approach Human-Engaged Computing (HEC). HEC is a focused yet adaptable philosophical approach which aims to establish “synergized interactions between engaged humans and engaging computers for high level wisdom which enhances our human survival probability and our full potential as humans”. In this paper, HEC is described through five perspectives—definition, components, principles, case studies and benefits. The paper concludes by suggesting future directions for HEC.",
        "publishYear": "2019",
        "authors":[{
            "name":"Xiangshi Ren"
        },{
            "name":"Chaklam Silpasuwanchai"
        }, {
            "name": "John Cahill"
        }],
        "relevancy":"8",
        "citing": "55",
        "cited" : "3"
    },
    {
        "_id" : "24",
        "title" : "Bringing the Field into the Lab: Large-Scale Visualization of Animal Movement Trajectories within a Virtual Island",
        "abstract" : "Animal behavior research is becoming an increasingly data-driven field, enabled by advancements in GPS tracking. Rather than directly observe movement and behavior in small numbers of animals, over short time-spans and in small areas, researchers can use GPS collars to track many animals, over large areas and long time spans. These large datasets have the potential to provide rich information about animal behavior. However, this tracking data needs to be integrated with geospatial data about the environment in order to put the movements and estimated behaviors into context. We present an immersive visualization which integrates high resolution topological data from Barro Colorado Island, a 4000 acre island located in the Panama Canal, with GPS tracking data from close to two dozen primates captured over several months. Our application leverages parallelization for rapid filtering of the movement data, allowing researchers to explore the data in a large-scale, immersive environment (CAVE2). We present this work in order to explore the possibility of creating virtual field environments from data, to bring the field into the lab and enable big data animal behavior research.",
        "publishYear": "2019",
        "authors":[
            {
                "name" : "Jillian Aurisano"
            },
            {
                "name" : "James Hwang"
            },
            {
                "name" : "Andrew Johnson"
            },
            {
                "name" : "Lance Long"
            },
            {
                "name" : "Margaret Crofoot"
            },
            {
                "name" : "Tanya Berger-Wolf"
            }
        ],
        "relevancy": "6",
        "citing": "11",
        "cited" : "1"
    },
    {
        "_id" : "25",
        "title" : "On the Impact of the Medium in the Effectiveness of 3D Software Visualizations",
        "abstract" : "Many visualizations have proven to be effective in supporting various software related tasks. Although multiple media can be used to display a visualization, the standard computer screen is used the most. We hypothesize that the medium has a role in their effectiveness. We investigate our hypotheses by conducting a controlled user experiment. In the experiment we focus on the 3D city visualization technique used for software comprehension tasks. We deploy 3D city visualizations across a standard computer screen (SCS), an immersive 3D environment (I3D), and a physical 3D printed model (P3D). We asked twenty-seven participants (whom we divided in three groups for each medium) to visualize software systems of various sizes, solve a set of uniform comprehension tasks, and complete a questionnaire. We measured the effectiveness of visualizations in terms of performance, recollection, and user experience. We found that even though developers using P3D required the least time to identify outliers, they perceived the least difficulty when visualizing systems based on SCS. Moreover, developers using I3D obtained the highest recollection.",
        "publishYear": "2017",
        "authors":[
            {
                "name" : "Leonel Merino"
            },
            {
                "name" : "Johannes Fuchs"
            },
            {
                "name" : "Michael Blumenschein"
            },
            {
                "name" : "Craig Anslow"
            },
            {
                "name" : "Mohammad Ghafari"
            },
            {
                "name" : "Oscar Nierstrasz"
            },
            {
                "name" : "Michael Behrisch"
            },
            {
                "name" : "Daniel A. Keim"
            }
        ],
        "relevancy": "7",
        "citing": "36",
        "cited" : "10"
    },
    {
        "_id" : "26",
        "title" : "Uncluttered Single-Image Visualization of Vascular Structures Using GPU and Integer Programming",
        "abstract" : "Direct projection of 3D branching structures, such as networks of cables, blood vessels, or neurons onto a 2D image creates the illusion of intersecting structural parts and creates challenges for understanding and communication. We present a method for visualizing such structures, and demonstrate its utility in visualizing the abdominal aorta and its branches, whose tomographic images might be obtained by computed tomography or magnetic resonance angiography, in a single 2D stylistic image, without overlaps among branches. The visualization method, termed uncluttered single-image visualization (USIV), involves optimization of geometry. This paper proposes a novel optimization technique that utilizes an interesting connection of the optimization problem regarding USIV to the protein structure prediction problem. Adopting the integer linear programming-based formulation for the protein structure prediction problem, we tested the proposed technique using 30 visualizations produced from five patient scans with representative anatomical variants in the abdominal aortic vessel tree. The novel technique can exploit commodity-level parallelism, enabling use of general-purpose graphics processing unit (GPGPU) technology that yields a significant speedup. Comparison of the results with the other optimization technique previously reported elsewhere suggests that, in most aspects, the quality of the visualization is comparable to that of the previous one, with a significant gain in the computation time of the algorithm.",
        "publishYear": "2012",
        "authors":[
            {
                "name" : "Joong-Ho Won"
            },
            {
                "name" : "Yongkweon Jeon"
            },
            {
                "name" : "Jarrett K. Rosenberg"
            },
            {
                "name" : "Sungroh Yoon"
            },
            {
                "name" : "Geoffrey D. Rubin"
            },
            {
                "name" : "Sandy Napel"
            }
        ],
        "relevancy": "4",
        "citing": "39",
        "cited" : "2"
    },
    {
        "_id" : "27",
        "title" : "Glyph-Based Comparative Visualization for Diffusion Tensor Fields",
        "abstract" : "Diffusion Tensor Imaging (DTI) is a magnetic resonance imaging modality that enables the in-vivo reconstruction and visualization of fibrous structures. To inspect the local and individual diffusion tensors, glyph-based visualizations are commonly used since they are able to effectively convey full aspects of the diffusion tensor. For several applications it is necessary to compare tensor fields, e.g., to study the effects of acquisition parameters, or to investigate the influence of pathologies on white matter structures. This comparison is commonly done by extracting scalar information out of the tensor fields and then comparing these scalar fields, which leads to a loss of information. If the glyph representation is kept, simple juxtaposition or superposition can be used. However, neither facilitates the identification and interpretation of the differences between the tensor fields. Inspired by the checkerboard style visualization and the superquadric tensor glyph, we design a new glyph to locally visualize differences between two diffusion tensors by combining juxtaposition and explicit encoding. Because tensor scale, anisotropy type, and orientation are related to anatomical information relevant for DTI applications, we focus on visualizing tensor differences in these three aspects. As demonstrated in a user study, our new glyph design allows users to efficiently and effectively identify the tensor differences. We also apply our new glyphs to investigate the differences between DTI datasets of the human brain in two different contexts using different b-values, and to compare datasets from a healthy and HIV-infected subject.",
        "publishYear": "2015",
        "authors":[
            {
                "name" : "Changgong Zhang"
            },
            {
                "name" : "Thomas Schultz"
            },
            {
                "name" : "Kai Lawonn"
            },
            {
                "name" : "Elmar Eisemann"
            },
            {
                "name" : "Anna Vilanova"
            }
        ],
        "relevancy": "7",
        "citing": "43",
        "cited" : "23"
    },
    {
        "_id" : "28",
        "title" : "An Enhanced Visualization Process Model for Incremental Visualization",
        "abstract" : "With today's technical possibilities, a stable visualization scenario can no longer be assumed as a matter of course, as underlying data and targeted display setup are much more in flux than in traditional scenarios. Incremental visualization approaches are a means to address this challenge, as they permit the user to interact with, steer, and change the visualization at intermediate time points and not just after it has been completed. In this paper, we put forward a model for incremental visualizations that is based on the established Data State Reference Model, but extends it in ways to also represent partitioned data and visualization operators to facilitate intermediate visualization updates. In combination, partitioned data and operators can be used independently and in combination to strike tailored compromises between output quality, shown data quantity, and responsiveness-i.e., frame rates. We showcase the new expressive power of this model by discussing the opportunities and challenges of incremental visualization in general and its usage in a real world scenario in particular.",
        "publishYear": "2015",
        "authors":[
            {
                "name" : "Hans-Jörg Schulz"
            },
            {
                "name" : "Marco Angelini"
            },
            {
                "name" : "Giuseppe Santucci"
            },
            {
                "name" : "Heidrun Schumann"
            }
        ],
        "relevancy": "6",
        "citing": "42",
        "cited" : "20"
    },
    {
        "_id" : "29",
        "title" : "Multiscale Time Activity Data Exploration via Temporal Clustering Visualization Spreadsheet",
        "abstract" : "Time-varying data is usually explored by animation or arrays of static images. Neither is particularly effective for classifying data by different temporal activities. Important temporal trends can be missed due to the lack of ability to find them with current visualization methods. In this paper, we propose a method to explore data at different temporal resolutions to discover and highlight data based upon time-varying trends. Using the wavelet transform along the time axis, we transform data points into multi-scale time series curve sets. The time curves are clustered so that data of similar activity are grouped together, at different temporal resolutions. The data are displayed to the user in a global time view spreadsheet where she is able to select temporal clusters of data points, and filter and brush data across temporal scales. With our method, a user can interact with data based on time activities and create expressive visualizations.",
        "publishYear": "2008",
        "authors":[
            {
                "name" : "Jonathan Woodring"
            },
            {
                "name" : "Han-Wei Shen"
            }
        ],
        "relevancy": "4",
        "citing": "31",
        "cited" : "42"
    },
    {
        "_id" : "30",
        "title" : "Noise-Based Volume Rendering for the Visualization of Multivariate Volumetric Data",
        "abstract" : "Analysis of multivariate data is of great importance in many scientific disciplines. However, visualization of 3D spatially-fixed multivariate volumetric data is a very challenging task. In this paper we present a method that allows simultaneous real-time visualization of multivariate data. We redistribute the opacity within a voxel to improve the readability of the color defined by a regular transfer function, and to maintain the see-through capabilities of volume rendering. We use predictable procedural noise - random-phase Gabor noise - to generate a high-frequency redistribution pattern and construct an opacity mapping function, which allows to partition the available space among the displayed data attributes. This mapping function is appropriately filtered to avoid aliasing, while maintaining transparent regions. We show the usefulness of our approach on various data sets and with different example applications. Furthermore, we evaluate our method by comparing it to other visualization techniques in a controlled user study. Overall, the results of our study indicate that users are much more accurate in determining exact data values with our novel 3D volume visualization method. Significantly lower error rates for reading data values and high subjective ranking of our method imply that it has a high chance of being adopted for the purpose of visualization of multivariate 3D data.",
        "publishYear": "2013",
        "authors":[
            {
                "name" : "Rostislav Khlebnikov"
            },
            {
                "name" : "Bernhard Kainz"
            },
            {
                "name" : "Markus Steinberger"
            },
            {
                "name" : "Dieter Schmalstieg"
            }
        ],
        "relevancy": "5",
        "citing": "29",
        "cited" : "9"
    },
    {
        "_id" : "31",
        "title" : "cpmViz: A Web-Based Visualization Tool for Uncertain Spatiotemporal Data",
        "abstract" : "The goal of the VAST challenge 2019 Mini Challenge 2 was to visualize radioactive contaminations measured by mobile and static sensors and their changes over time, allowing city officials to determine the severity of the leakage at the city's nuclear power plant. We propose cpmViz, a web-based tool that allows for interactive data exploration of the sensor readings in both of the spatial and temporal dimensions. The tool consists out of three views that are connected via linking and scrolling. We visualize static sensor uncertainty by introducing Voronoi cells to illustrate how much space is covered by an individual measurement unit. For mobile sensors, we showcase their activity periods and introduce the concept of sensor streaks as periods of uninterrupted recordings as a temporal uncertainty measure. As for spatial uncertainty, we color individual districts based on the amount of data that was recorded inside the user's selected time window. Using our system, we were able to easily spot major events like the city's initial earthquake in the sensor readings. Certain southern districts are clearly visible as areas of concern that we consider in need of more static sensors. Furthermore, we were also able to identify static as well as moving contaminations.",
        "publishYear": "2019",
        "authors":[
            {
                "name" : "Fabian Nagel"
            },
            {
                "name" : "Giuliano Castiglia"
            },
            {
                "name" : "Gemza Ademaj"
            },
            {
                "name" : "Juri Buchmller"
            },
            {
                "name" : "Udo Schlegel"
            },
            {
                "name" : "Daniel A. Keim"
            }
        ],
        "relevancy": "7",
        "citing": "0",
        "cited" : "0"
    },
    {
        "_id" : "32",
        "title" : "Uncertainty Visualization for Secondary Structures of Proteins",
        "abstract" : "We present a technique that conveys the uncertainty in the secondary structure of proteins-an abstraction model based on atomic coordinates. While protein data inherently contains uncertainty due to the acquisition method or the simulation algorithm, we argue that it is also worth investigating uncertainty induced by analysis algorithms that precede visualization. Our technique helps researchers investigate differences between multiple secondary structure assignment methods. We modify established algorithms for fuzzy classification and introduce a discrepancy-based approach to project an ensemble of sequences to a single importance-weighted sequence. In 2D, we depict the aggregated secondary structure assignments based on the per-residue deviation in a collapsible sequence diagram. In 3D, we extend the ribbon diagram using visual variables such as transparency, wave form, frequency, or amplitude to facilitate qualitative analysis of uncertainty. We evaluated the effectiveness and acceptance of our technique through expert reviews using two example applications: the combined assignment against established algorithms and time-dependent structural changes originating from simulated protein dynamics.",
        "publishYear": "2018",
        "authors":[
            {
                "name" : "Christoph Schulz"
            },
            {
                "name" : "Karsten Schatz"
            },
            {
                "name" : "Michael Krone"
            },
            {
                "name" : "Matthias Braun"
            },
            {
                "name" : "Thomas Ertl"
            },
            {
                "name" : "Daniel Weiskopf"
            }
        ],
        "relevancy": "4",
        "citing": "47",
        "cited" : "2"
    },
    {
        "_id" : "33",
        "title" : "Learning Visualizations by Analogy: Promoting Visual Literacy through Visualization Morphing",
        "abstract" : "We propose the concept of teaching (and learning) unfamiliar visualizations by analogy, that is, demonstrating an unfamiliar visualization method by linking it to another more familiar one, where the in-betweens are designed to bridge the gap of these two visualizations and explain the difference in a gradual manner. As opposed to a textual description, our morphing explains an unfamiliar visualization through purely visual means. We demonstrate our idea by ways of four visualization pair examples: data table and parallel coordinates, scatterplot matrix and hyperbox, linear chart and spiral chart, and hierarchical pie chart and treemap. The analogy is commutative i.e. any member of the pair can be the unfamiliar visualization. A series of studies showed that this new paradigm can be an effective teaching tool. The participants could understand the unfamiliar visualization methods in all of the four pairs either fully or at least significantly better after they observed or interacted with the transitions from the familiar counterpart. The four examples suggest how helpful visualization pairings be identified and they will hopefully inspire other visualization morphings and associated transition strategies to be identified.",
        "publishYear": "2015",
        "authors":[
            {
                "name" : "Puripant Ruchikachorn"
            },
            {
                "name" : "Klaus Mueller"
            }
        ],
        "relevancy": "7",
        "citing": "50",
        "cited" : "18"
    },
    {
        "_id" : "34",
        "title" : "The Explanatory Visualization Framework: An Active Learning Framework for Teaching Creative Computing Using Explanatory Visualizations",
        "abstract" : "Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.",
        "publishYear": "2017",
        "authors":[
            {
                "name" : "Jonathan C. Roberts"
            },
            {
                "name" : "Panagiotis D. Ritsos"
            },
            {
                "name" : "James R. Jackson"
            },
            {
                "name" : "Christopher Headleand"
            }
        ],
        "relevancy": "7",
        "citing": "70",
        "cited" : "2"
    }
]